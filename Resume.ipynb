{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3beb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score,roc_auc_score,confusion_matrix,classification_report,roc_curve\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ae6f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edish\\AppData\\Local\\Temp\\ipykernel_984\\819538062.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_cv = df_cv.append({'cv': text}, ignore_index=True)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "no such file: 'C:\\Users\\Edish\\Documents\\Python Taskilled\\Project Mentorness\\Resumes\\barry_allen_fe.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cv \u001b[38;5;129;01min\u001b[39;00m cv_files:\n\u001b[0;32m      6\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mEdish\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPython Taskilled\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mProject Mentorness\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mResumes\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     document \u001b[38;5;241m=\u001b[39m \u001b[43mfitz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Hər səhifənin mətnini oxumaq\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pymupdf\\__init__.py:2721\u001b[0m, in \u001b[0;36mDocument.__init__\u001b[1;34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[0m\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[0;32m   2720\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno such file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m   2722\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[0;32m   2723\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is no file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: no such file: 'C:\\Users\\Edish\\Documents\\Python Taskilled\\Project Mentorness\\Resumes\\barry_allen_fe.pdf'"
     ]
    }
   ],
   "source": [
    "# PDF faylını oxumaq\n",
    "cv_files = ['alfred_pennyworth_pm.pdf', 'barry_allen_fe.pdf','bruce_wayne_fullstack.pdf','john_doe.pdf','harvey_dent_mle.pdf']\n",
    "df_cv = pd.DataFrame(columns=['cv'])\n",
    "\n",
    "for cv in cv_files:\n",
    "    file_path = f'C:\\\\Users\\\\Edish\\\\Documents\\\\Python Taskilled\\\\Project Mentorness\\\\Resumes\\\\{cv}'\n",
    "\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    # Hər səhifənin mətnini oxumaq\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "        \n",
    "    df_cv = df_cv.append({'cv': text}, ignore_index=True)\n",
    "  \n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9807243",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PDF faylını oxumaq\n",
    "job_files = ['job_desc_front_end_engineer.pdf', 'job_desc_full_stack_engineer.pdf','job_desc_java_developer.pdf','job_desc_product_manager.pdf','ml vacancies.pdf']\n",
    "df_job = pd.DataFrame(columns=['job'])\n",
    "\n",
    "for job in job_files:\n",
    "    file_path = f'C:\\\\Users\\\\Edish\\\\Documents\\\\Python Taskilled\\\\Project Mentorness\\\\JobDescription\\\\{job}'\n",
    "\n",
    "\n",
    "    document = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    # Hər səhifənin mətnini oxumaq\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "        \n",
    "    df_job = df_job.append({'job': text}, ignore_index=True)\n",
    "  \n",
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords yüklə\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d3d00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Mətn təmizləmə funksiyası (stop words çıxarılması daxil)\n",
    "def clean_text_cv(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\d{3,}\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    match = re.search(r'(objective|professional summary)', text, re.IGNORECASE)\n",
    "    if text.strip()[-1].isdigit():\n",
    "        text = re.sub(r'\\d+\\s*$', '', text).strip()\n",
    "    if match:\n",
    "        return text[match.end():]  # Başlıqdan sonrakı mətn hissəsini qaytar\n",
    "    return text  \n",
    "\n",
    "    # Stop words sil\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    # Lemmatization\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())    \n",
    "\n",
    "def clean_text_job(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b\\d{3,}\\b', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    match = re.search(r'(how to apply)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return text[:match.start()]  # Başlıqdan əvvəlki mətn hissəsini qaytar\n",
    "    return text  # Əgər başlıq tapılmazsa, bütün mətni qaytar\n",
    "\n",
    "    # Stop words sil\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    # Lemmatization\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "\n",
    "df_cv['clean_cv'] = df_cv['cv'].apply(clean_text_cv)\n",
    "df_job['clean_job'] = df_job['job'].apply(clean_text_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107ddbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_job['clean_job'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c066ec33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cv['clean_cv'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7364bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_texts = df_cv['clean_cv'].tolist()\n",
    "job_texts = df_job['clean_job'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951777b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Modeli yükləyin\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "# CV və iş təsvirlərini vektorlaşdirin\n",
    "cv_embeddings = model.encode(cv_texts, convert_to_tensor=True)\n",
    "job_embeddings = model.encode(job_texts, convert_to_tensor=True)\n",
    "\n",
    "# Cosine Similarity hesablayın\n",
    "similarity_scores = util.pytorch_cos_sim(cv_embeddings, job_embeddings)\n",
    "print(\"Cosine Similarity Scores with BERT:\", similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993858dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_scores\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m similarity_scores[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'similarity_scores' is not defined"
     ]
    }
   ],
   "source": [
    "similarity_scores = similarity_scores.reshape(-1,1)\n",
    "similarity_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c9dc799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5  # Uyğunluq skoru üçün threshold\n",
    "y = []\n",
    "\n",
    "# Cosine similarity matrisindən etiketi təyin etmək\n",
    "for row in similarity_scores:\n",
    "    row_labels = [1 if score >= threshold else 0 for score in row]\n",
    "    y.extend(row_labels)\n",
    "\n",
    "y = np.array(y)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5aec9b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1]) torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(similarity_scores, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2c19f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model = None,train_features = None,train_labels = None,test_features = None,test_labels =None,algorithm_name = None):\n",
    "    \n",
    "    accuracy = accuracy_score(y_true = test_labels, y_pred = model.predict(X = test_features))\n",
    "    precision = precision_score(y_true = test_labels, y_pred = model.predict(X = test_features))\n",
    "    recall = recall_score(y_true = test_labels, y_pred = model.predict(X = test_features))\n",
    "    f1 = f1_score(y_true= test_labels, y_pred= model.predict(X = test_features))\n",
    "    auc = roc_auc_score(y_true= test_labels, y_score = model.predict(X = test_features))\n",
    "    \n",
    "    print(f\"Model Performance for:{algorithm_name}\\n\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3d0d2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for:Logistic Regression\n",
      "\n",
      "Accuracy: 0.8\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edish\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lg_classifier = LogisticRegression()\n",
    "lg_classifier = lg_classifier.fit(X_train, y_train)\n",
    "\n",
    "model_summary = evaluate_model_performance(model = lg_classifier, \n",
    "                                              train_features = X_train, \n",
    "                                              train_labels = y_train, \n",
    "                                              test_features = X_test, \n",
    "                                              test_labels = y_test,\n",
    "                                              algorithm_name = 'Logistic Regression')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c41343aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for:SVM model\n",
      "\n",
      "Accuracy: 0.8\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n",
      "AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edish\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# SVM modelinin qurulması\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Modelin təlimi\n",
    "svm_model = svm_model.fit(X_train, y_train)\n",
    "\n",
    "model_summary = evaluate_model_performance(model = svm_model, \n",
    "                                              train_features = X_train, \n",
    "                                              train_labels = y_train, \n",
    "                                              test_features = X_test, \n",
    "                                              test_labels = y_test,\n",
    "                                              algorithm_name = 'SVM model')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0c51c309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for:XGB Model\n",
      "\n",
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n",
      "AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier()\n",
    "xgb_classifier = xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "model_summary = evaluate_model_performance(model = xgb_classifier, \n",
    "                                              train_features = X_train, \n",
    "                                              train_labels = y_train, \n",
    "                                              test_features = X_test, \n",
    "                                              test_labels = y_test,\n",
    "                                              algorithm_name = 'XGB Model')\n",
    "model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9df80ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file = 'best_model.pickle',mode = 'wb') as pickled_file:\n",
    "    pickle.dump(obj = svm_model,file = pickled_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980af60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c1426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2824ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Weighted Cosine Similarity Scores: [[0.5499819  0.52754265 0.67972195 0.4689391  0.5760403 ]\n",
      " [0.5874162  0.4211997  0.4903461  0.28018028 0.5129319 ]\n",
      " [0.5718806  0.5870232  0.5381878  0.41839695 0.6218357 ]\n",
      " [0.4910674  0.42190823 0.5645016  0.43982476 0.5834123 ]\n",
      " [0.4977513  0.5088762  0.5290378  0.34404066 0.60508555]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "# # TF-IDF dəyərlərini əldə edin\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# tfidf_matrix = vectorizer.fit_transform(cv_texts + job_texts)\n",
    "# words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# # Sözlərə uyğun vektorları alın\n",
    "# def get_tfidf_weighted_vector(text, model, tfidf, words):\n",
    "#     words = text.split()\n",
    "#     vectors = [model.wv[word] * tfidf[words.index(word)] for word in words if word in model.wv and word in words]\n",
    "#     return np.mean(vectors, axis=0)\n",
    "\n",
    "# # Vektorları əldə edin\n",
    "# tfidf_weights = tfidf_matrix.toarray()\n",
    "# weighted_vectors = [get_tfidf_weighted_vector(text, model, tfidf_weights[i], words) for i, text in enumerate(cv_texts + job_texts)]\n",
    "\n",
    "# # Cosine Similarity hesablayın\n",
    "# similarity_scores = cosine_similarity(weighted_vectors[:len(cv_texts)], weighted_vectors[len(cv_texts):])\n",
    "# print(\"TF-IDF Weighted Cosine Similarity Scores:\", similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e9fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e83ad042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2713236  0.2952416  0.34659774 0.38327186 0.29512025]\n",
      " [0.25405963 0.19422723 0.21242943 0.15701689 0.21376736]\n",
      " [0.30882144 0.33344599 0.34866195 0.32795125 0.34520656]\n",
      " [0.29494089 0.32418303 0.44685011 0.27817898 0.30080672]\n",
      " [0.20852865 0.21122893 0.22946584 0.22655039 0.49530609]]\n"
     ]
    }
   ],
   "source": [
    "# # CV və iş təsvirlərini list şəklinə çevirin\n",
    "# cv_texts = df_cv['clean_cv'].tolist()\n",
    "# job_texts = df_job['clean_job'].tolist()\n",
    "\n",
    "# # TfidfVectorizer obyektini yaradın\n",
    "# vectorizer = TfidfVectorizer()\n",
    "\n",
    "# # CV və iş təsvirlərini birləşdirin və uyğunlaşdırın\n",
    "# combined_texts = cv_texts + job_texts\n",
    "# combined_tfidf = vectorizer.fit_transform(combined_texts)\n",
    "\n",
    "# # CV və iş təsvirləri üçün vektorları ayırın\n",
    "# cv_tfidf = combined_tfidf[:len(cv_texts)]\n",
    "# job_tfidf = combined_tfidf[len(cv_texts):]\n",
    "\n",
    "# #cv_tfidf = cv_tfidf.toarray()\n",
    "# #job_tfidf = job_tfidf.toarray()\n",
    "# # CV və iş təsvirləri arasında oxşarlığı hesablayırıq\n",
    "# similarity_scores = cosine_similarity(cv_tfidf, job_tfidf)\n",
    "\n",
    "# # Nəticələri göstərmək\n",
    "# print(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a604e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a392324",
   "metadata": {},
   "source": [
    "### Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "531fbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# # ALBERT tokenləşdirmə metodunu yükləyin\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# cv_texts = df_cv['clean_cv'].tolist()\n",
    "# job_texts = df_job['clean_job'].tolist()\n",
    "\n",
    "# # CV və vakansiya mətnlərini tokenləşdirin\n",
    "# cv_tokens = tokenizer(cv_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "# job_tokens = tokenizer(job_texts, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cb036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "# import torch\n",
    "\n",
    "# # BERT modelini yükləyin\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # CV və vakansiya tokenlərini BERT modelinə verərək vektorlara çevirin\n",
    "# with torch.no_grad():\n",
    "#     cv_outputs = bert_model(**cv_tokens)\n",
    "#     job_outputs = bert_model(**job_tokens)\n",
    "\n",
    "# # CV və vakansiya vektorlarını əldə edin\n",
    "# cv_embedding = cv_outputs.last_hidden_state.mean(dim=1)\n",
    "# job_embedding = job_outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3530dec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uyğunluq dərəcəsi: [[0.94543344 0.9649761  0.9687836  0.967291   0.9602374 ]\n",
      " [0.9631349  0.9719155  0.9696667  0.9667721  0.9264314 ]\n",
      " [0.93799853 0.953841   0.9735334  0.9519393  0.9451593 ]\n",
      " [0.958897   0.9707974  0.9795161  0.9594753  0.9303658 ]\n",
      " [0.9550263  0.97017306 0.97079575 0.9645685  0.9495796 ]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # CV və vakansiya vektorları arasında kosinus bənzərliyini hesablamaq\n",
    "# similarity = cosine_similarity(cv_embedding, job_embedding)\n",
    "\n",
    "# print(\"Uyğunluq dərəcəsi:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300d8adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8216241f",
   "metadata": {},
   "source": [
    "### Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "529cd809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AlbertTokenizer, AlbertModel\n",
    "\n",
    "# # ALBERT tokenləşdirmə metodunu yükləyin\n",
    "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "# cv_texts = df_cv['clean_cv'].tolist()\n",
    "# job_texts = df_job['clean_job'].tolist()\n",
    "\n",
    "# # CV və vakansiya mətnlərini tokenləşdirin\n",
    "# cv_tokens = tokenizer(cv_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "# job_tokens = tokenizer(job_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c966b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AlbertModel\n",
    "# import torch\n",
    "\n",
    "# # BERT modelini yükləyin\n",
    "# model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "\n",
    "# # CV və vakansiya tokenlərini BERT modelinə verərək vektorlara çevirin\n",
    "# with torch.no_grad():\n",
    "#     cv_outputs = model(**cv_tokens)\n",
    "#     job_outputs = model(**job_tokens)\n",
    "\n",
    "# # CV və vakansiya vektorlarını əldə edin\n",
    "# cv_embedding = cv_outputs.last_hidden_state.mean(dim=1)\n",
    "# job_embedding = job_outputs.last_hidden_state.mean(dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc6435a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uyğunluq dərəcəsi: [[0.9538876  0.97145283 0.9490682  0.9583086 ]\n",
      " [0.9093657  0.9410251  0.87654054 0.91929376]\n",
      " [0.95489985 0.92184323 0.95910424 0.89633787]\n",
      " [0.94784176 0.9595026  0.9431582  0.9293703 ]\n",
      " [0.9348298  0.9496388  0.92542815 0.9364503 ]]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # CV və vakansiya vektorları arasında kosinus bənzərliyini hesablamaq\n",
    "# similarity = cosine_similarity(cv_embedding, job_embedding)\n",
    "\n",
    "# print(\"Uyğunluq dərəcəsi:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d62709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "389a8111",
   "metadata": {},
   "source": [
    "### Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dffa68ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores with Word2Vec: [[0.9556517  0.94924647 0.96595997 0.94669056 0.9490228 ]\n",
      " [0.9394299  0.93133664 0.9341774  0.91734827 0.9365722 ]\n",
      " [0.9540197  0.954368   0.9597762  0.9509833  0.9524786 ]\n",
      " [0.9502     0.9455668  0.9580271  0.9345489  0.9474186 ]\n",
      " [0.9442921  0.9484159  0.9499204  0.94843334 0.95516396]]\n"
     ]
    }
   ],
   "source": [
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# # Mətni cümlələrə böl\n",
    "# sentences = [text.split() for text in cv_texts + job_texts]\n",
    "\n",
    "# # Word2Vec modelini öyrət\n",
    "# model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# # Vektorizasiya funksiyası\n",
    "# def get_vector(text):\n",
    "#     words = text.split()\n",
    "#     vector = [model.wv[word] for word in words if word in model.wv]\n",
    "#     return np.mean(vector, axis=0)\n",
    "\n",
    "# # Vektorları əldə et\n",
    "# cv_vectors = np.array([get_vector(text) for text in cv_texts])\n",
    "# job_vectors = np.array([get_vector(text) for text in job_texts])\n",
    "\n",
    "# # Cosine Similarity hesablayın\n",
    "# similarity_scores = cosine_similarity(cv_vectors, job_vectors)\n",
    "# print(\"Cosine Similarity Scores with Word2Vec:\", similarity_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd08eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
